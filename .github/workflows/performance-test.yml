name: Performance Testing

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:
    inputs:
      url:
        description: 'URL to test (optional, defaults to localhost)'
        required: false
        default: 'http://localhost:3000'

jobs:
  performance-test:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - name: Install dependencies
        run: npm ci
        env:
          NUXT_TELEMETRY_DISABLED: '1'
          CI: 'true'

      - name: Build application
        run: npm run build
        env:
          NUXT_TELEMETRY_DISABLED: '1'
          CI: 'true'

      - name: Create performance budget
        run: |
          # Use the committed budget file or create a default one
          if [ ! -f ".github/performance-budget.json" ]; then
            cat > budget.json << 'EOF'
          {
            "budget": {
              "score": {
                "performance": 90
              },
              "lighthouse": {
                "performance": 90
              },
              "timings": {
                "firstContentfulPaint": 2000,
                "largestContentfulPaint": 4000,
                "SpeedIndex": 3000
              },
              "requests": {
                "total": 100
              },
              "transferSize": {
                "total": 2000000
              }
            }
          }
          EOF
          else
            cp .github/performance-budget.json budget.json
          fi

      - name: Set target URL
        id: url
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ github.event.inputs.url }}" ]; then
            echo "target_url=${{ github.event.inputs.url }}" >> $GITHUB_OUTPUT
          else
            echo "target_url=http://localhost:3000" >> $GITHUB_OUTPUT
          fi

      - name: Start application in background
        run: |
          echo "Starting Nuxt application..."
          npm run preview &
          APP_PID=$!
          echo "APP_PID=$APP_PID" >> $GITHUB_ENV
          echo "Application started with PID: $APP_PID"

      - name: Wait for application to be ready
        run: |
          echo "Waiting for application to be ready..."
          for i in {1..60}; do
            if curl -sSf "${{ steps.url.outputs.target_url }}" > /dev/null 2>&1; then
              echo "Application is responding!"
              sleep 5  # Give it a few more seconds to fully initialize
              break
            fi
            echo "Attempt $i: Application not ready, waiting 5 seconds..."
            sleep 5
          done
          
          # Final check
          if ! curl -sSf "${{ steps.url.outputs.target_url }}" > /dev/null 2>&1; then
            echo "âŒ Application failed to start properly"
            exit 1
          fi
          echo "âœ… Application is ready for testing"

      - name: Run sitespeed.io performance test
        id: performance_test
        run: |
          # Create output directory
          mkdir -p performance-results
          
          # Run sitespeed.io with Docker
          # Use host networking to access localhost from container
          docker run --rm \
            --network="host" \
            -v "$(pwd):/sitespeed.io" \
            sitespeedio/sitespeed.io:38.1.1 \
            "${{ steps.url.outputs.target_url }}" \
            --budget.configPath budget.json \
            --budget.output json \
            --outputFolder performance-results \
            --plugins.remove lighthouse \
            --plugins.add lighthouse \
            --lighthouse.settings.onlyCategories performance \
            --browsertime.iterations 3 \
            --browser chrome \
            --mobile \
            -d 2 \
            --suppressDomainFolder \
            --gzipHAR \
            --browsertime.headless \
            || echo "performance_failed=true" >> $GITHUB_OUTPUT

      - name: Stop application
        if: always()
        run: |
          if [ -n "$APP_PID" ]; then
            echo "Stopping application with PID: $APP_PID"
            kill $APP_PID || true
            # Also kill any remaining node processes
            pkill -f "node.*nuxt" || true
          fi

      - name: Install bc for calculations
        run: sudo apt-get update && sudo apt-get install -y bc

      - name: Parse performance results
        id: parse_results
        run: |
          if [ -f "performance-results/budgetResult.json" ]; then
            echo "ğŸ“„ Budget results file found, parsing..."
            
            # Debug: Show the structure of the JSON file
            echo "ğŸ“‹ Budget results structure:"
            cat performance-results/budgetResult.json | jq '.' || echo "âŒ Invalid JSON format"
            
            # Parse the budget results with the correct structure
            budget_results=$(cat performance-results/budgetResult.json)
            
            # Count working and failing tests for all URLs
            working_tests=0
            failing_tests=0
            
            # Count working tests
            if echo "$budget_results" | jq -e '.working' > /dev/null; then
              for url in $(echo "$budget_results" | jq -r '.working | keys[]'); do
                url_working=$(echo "$budget_results" | jq ".working[\"$url\"] | length")
                working_tests=$((working_tests + url_working))
              done
            fi
            
            # Count failing tests
            if echo "$budget_results" | jq -e '.failing' > /dev/null; then
              for url in $(echo "$budget_results" | jq -r '.failing | keys[]'); do
                url_failing=$(echo "$budget_results" | jq ".failing[\"$url\"] | length")
                failing_tests=$((failing_tests + url_failing))
              done
            fi
            
            total_tests=$((working_tests + failing_tests))
            passed_tests=$working_tests
            
            # Calculate pass percentage
            if [ "$total_tests" -gt 0 ]; then
              pass_percentage=$(echo "scale=2; $passed_tests * 100 / $total_tests" | bc -l)
            else
              pass_percentage=0
            fi
            
            echo "ğŸ“Š Results: $passed_tests/$total_tests tests passed ($pass_percentage%)"
            echo "âœ… Working tests: $working_tests"
            echo "âŒ Failing tests: $failing_tests"
            
            echo "total_tests=$total_tests" >> $GITHUB_OUTPUT
            echo "passed_tests=$passed_tests" >> $GITHUB_OUTPUT
            echo "failed_tests=$failing_tests" >> $GITHUB_OUTPUT
            echo "pass_percentage=$pass_percentage" >> $GITHUB_OUTPUT
            
            # Check if we meet the 90% threshold
            if (( $(echo "$pass_percentage >= 90" | bc -l) )); then
              echo "performance_passed=true" >> $GITHUB_OUTPUT
              echo "ğŸ‰ Performance test PASSED with $pass_percentage% of tests passing"
            else
              echo "performance_passed=false" >> $GITHUB_OUTPUT
              echo "âš ï¸ Performance test FAILED with $pass_percentage% of tests passing (threshold: 90%)"
              
              # Show details of failing tests
              echo ""
              echo "ğŸ” Failing tests details:"
              if echo "$budget_results" | jq -e '.failing' > /dev/null; then
                for url in $(echo "$budget_results" | jq -r '.failing | keys[]'); do
                  echo "URL: $url"
                  echo "$budget_results" | jq -r ".failing[\"$url\"][] | \"  - \(.metric): \(.friendlyValue) (limit: \(.friendlyLimit))\""
                done
              fi
            fi
          else
            echo "âŒ No budget results found at performance-results/budgetResult.json"
            echo "ğŸ“ Checking what files were created:"
            ls -la performance-results/ || echo "Performance results directory not found"
            echo "performance_passed=false" >> $GITHUB_OUTPUT
            echo "total_tests=0" >> $GITHUB_OUTPUT
            echo "passed_tests=0" >> $GITHUB_OUTPUT
            echo "failed_tests=0" >> $GITHUB_OUTPUT
            echo "pass_percentage=0" >> $GITHUB_OUTPUT
          fi

      - name: Generate performance summary
        run: |
          echo "## ğŸš€ Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Target URL:** ${{ steps.url.outputs.target_url }}" >> $GITHUB_STEP_SUMMARY
          echo "**Test Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.parse_results.outputs.performance_passed }}" = "true" ]; then
            echo "### âœ… Performance Budget: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "### âŒ Performance Budget: FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total Tests | ${{ steps.parse_results.outputs.total_tests }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Passed Tests | ${{ steps.parse_results.outputs.passed_tests }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Failed Tests | ${{ steps.parse_results.outputs.failed_tests }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Pass Percentage | ${{ steps.parse_results.outputs.pass_percentage }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| Required Threshold | 90% |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "performance-results/index.html" ]; then
            echo "ğŸ“Š **Detailed performance report available in artifacts**" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results
          path: performance-results/
          retention-days: 30

      - name: Comment on PR (if applicable)
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const passPercentage = parseFloat('${{ steps.parse_results.outputs.pass_percentage }}');
            const passed = '${{ steps.parse_results.outputs.performance_passed }}' === 'true';
            const emoji = passed ? 'âœ…' : 'âŒ';
            const status = passed ? 'PASSED' : 'FAILED';
            
            // Read the budget results for detailed information
            let detailedResults = '';
            try {
              const budgetData = JSON.parse(fs.readFileSync('performance-results/budgetResult.json', 'utf8'));
              
              // Get all tested URLs/routes
              const allUrls = new Set();
              if (budgetData.working) Object.keys(budgetData.working).forEach(url => allUrls.add(url));
              if (budgetData.failing) Object.keys(budgetData.failing).forEach(url => allUrls.add(url));
              
              // Add routes information
              if (allUrls.size > 0) {
                detailedResults += '\n### ğŸŒ Tested Routes\n\n';
                detailedResults += '| Route | Total Tests | Passed | Failed | Pass Rate |\n';
                detailedResults += '|-------|-------------|--------|--------|-----------|\n';
                
                for (const url of Array.from(allUrls).sort()) {
                  const workingCount = budgetData.working?.[url]?.length || 0;
                  const failingCount = budgetData.failing?.[url]?.length || 0;
                  const total = workingCount + failingCount;
                  const passRate = total > 0 ? Math.round((workingCount / total) * 100) : 0;
                  const routeName = url.replace('http://localhost:3000', '') || '/';
                  const statusIcon = passRate === 100 ? 'âœ…' : passRate >= 90 ? 'âš ï¸' : 'âŒ';
                  
                  detailedResults += `| \`${routeName}\` | ${total} | ${workingCount} | ${failingCount} | ${statusIcon} ${passRate}% |\n`;
                }
                detailedResults += '\n';
              }
              
              // Add working tests summary with route context
              if (budgetData.working && Object.keys(budgetData.working).length > 0) {
                detailedResults += '\n### âœ… Passing Tests by Route\n\n';
                for (const url of Object.keys(budgetData.working).sort()) {
                  const workingTests = budgetData.working[url];
                  const routeName = url.replace('http://localhost:3000', '') || '/';
                  detailedResults += `<details><summary>ğŸ“Š Route \`${routeName}\` - ${workingTests.length} tests passing</summary>\n\n`;
                  detailedResults += '| Metric | Current Value | Limit | Type |\n';
                  detailedResults += '|--------|---------------|-------|------|\n';
                  workingTests.forEach(test => {
                    detailedResults += `| ${test.metric} | ${test.friendlyValue} | ${test.friendlyLimit} | ${test.type} |\n`;
                  });
                  detailedResults += '\n</details>\n\n';
                }
              }
              
              // Add failing tests details with route context
              if (budgetData.failing && Object.keys(budgetData.failing).length > 0) {
                detailedResults += '\n### âŒ Failing Tests by Route\n\n';
                for (const url of Object.keys(budgetData.failing).sort()) {
                  const failingTests = budgetData.failing[url];
                  const routeName = url.replace('http://localhost:3000', '') || '/';
                  detailedResults += `#### ğŸš¨ Route \`${routeName}\` - ${failingTests.length} test(s) failing\n\n`;
                  detailedResults += '| Metric | Current Value | Limit | Exceeded By | Type |\n';
                  detailedResults += '|--------|---------------|-------|-------------|------|\n';
                  failingTests.forEach(test => {
                    let exceededBy = '';
                    if (test.limitType === 'max') {
                      const current = typeof test.value === 'number' ? test.value : 0;
                      const limit = typeof test.limit === 'number' ? test.limit : 0;
                      const diff = current - limit;
                      if (test.type === 'transferSize') {
                        exceededBy = `+${(diff / 1000).toFixed(1)} KB`;
                      } else if (test.metric.includes('Time') || test.metric.includes('Paint')) {
                        exceededBy = `+${diff} ms`;
                      } else {
                        exceededBy = `+${diff}`;
                      }
                    } else if (test.limitType === 'min') {
                      const current = typeof test.value === 'number' ? test.value : 0;
                      const limit = typeof test.limit === 'number' ? test.limit : 0;
                      const diff = limit - current;
                      exceededBy = `-${diff} points`;
                    }
                    detailedResults += `| **${test.metric}** | ${test.friendlyValue} | ${test.friendlyLimit} | ${exceededBy} | ${test.type} |\n`;
                  });
                  detailedResults += '\n';
                }
              }
              
              // Add performance categories summary
              if (budgetData.working || budgetData.failing) {
                detailedResults += '\n### ğŸ“Š Performance Categories (All Routes Combined)\n\n';
                const categories = {};
                
                // Count by type across all routes
                if (budgetData.working) {
                  Object.values(budgetData.working).flat().forEach(test => {
                    categories[test.type] = categories[test.type] || { pass: 0, fail: 0 };
                    categories[test.type].pass++;
                  });
                }
                if (budgetData.failing) {
                  Object.values(budgetData.failing).flat().forEach(test => {
                    categories[test.type] = categories[test.type] || { pass: 0, fail: 0 };
                    categories[test.type].fail++;
                  });
                }
                
                detailedResults += '| Category | Passed | Failed | Total | Pass Rate |\n';
                detailedResults += '|----------|--------|--------|-------|----------|\n';
                for (const [type, counts] of Object.entries(categories)) {
                  const total = counts.pass + counts.fail;
                  const percentage = Math.round((counts.pass / total) * 100);
                  const statusEmoji = percentage === 100 ? 'âœ…' : percentage >= 80 ? 'âš ï¸' : 'âŒ';
                  detailedResults += `| ${type} | ${counts.pass} | ${counts.fail} | ${total} | ${statusEmoji} ${percentage}% |\n`;
                }
              }
              
            } catch (error) {
              detailedResults = '\n> âš ï¸ Could not parse detailed results from budget file.\n';
            }
            
            const body = `## ${emoji} Performance Test Results
            
            **Status:** ${status} (${passPercentage}% of tests passing)  
            **Threshold:** 90% (${passed ? 'Met' : 'Not met'})  
            **URL Tested:** ${{ steps.url.outputs.target_url }} (local build)  
            **Test Date:** ${new Date().toISOString().split('T')[0]}
            
            ### ğŸ“ˆ Summary
            | Metric | Value |
            |--------|-------|
            | Total Tests | ${{ steps.parse_results.outputs.total_tests }} |
            | Passed | ${{ steps.parse_results.outputs.passed_tests }} |
            | Failed | ${{ steps.parse_results.outputs.failed_tests }} |
            | Pass Rate | ${passPercentage}% |
            | Status | ${passed ? 'âœ… PASSED' : 'âŒ FAILED'} |
            ${detailedResults}
            
            ### ï¿½ Additional Resources
            ï¿½ğŸ“Š [View detailed performance report in artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ---
            > ğŸ’¡ **Note:** This is a non-blocking test. The PR can be merged regardless of performance results.
            > 
            > ${passed ? 'ğŸ‰ Great job! Your changes maintain excellent performance across all tested routes.' : 'ğŸ”§ Consider optimizing the failing metrics before merging to improve overall performance.'}`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Final status check
        run: |
          if [ "${{ steps.parse_results.outputs.performance_passed }}" = "true" ]; then
            echo "ğŸ‰ Performance test passed! Local build meets the 90% performance threshold."
            exit 0
          else
            echo "âš ï¸ Performance test failed, but this is non-blocking. Check the results above."
            echo "The build completed successfully regardless of this result."
            # Exit with 0 to make this non-blocking
            exit 0
          fi
